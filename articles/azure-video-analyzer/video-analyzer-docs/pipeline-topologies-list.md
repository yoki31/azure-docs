---
title: List of pipeline topologies
description: This article lists validated sample pipeline topologies for Azure Video Analyzer. 
ms.topic: conceptual
ms.date: 01/12/2022
---

# List of pipeline topologies

The following tables list validated sample Azure Video Analyzer [live pipeline topologies](terminology.md#pipeline-topology). These topologies can be further customized according to solution needs. The tables also provide

* A short description,
* Topology's corresponding sample tutorial(s), and 
* The corresponding pipeline topology name of the Visual Studio Code (VSCode) [Video Analyzer extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.azure-video-analyzer). 

Clicking on a topology name redirects to the corresponding JSON file located in [this GitHub folder](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/), clicking on a sample redirects to the corresponding sample document, and clicking on a VSCode name redirects to a screenshot of the sample topology.

## Live pipeline topologies

### Continuous video recording

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[cvr-video-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/cvr-video-sink/topology.json) | Perform continuous video recording (CVR). Capture video and continuously record it to an Azure Video Analyzer video. | [Continuous video recording and playback](edge/use-continuous-video-recording.md) | [Record to Video Analyzer video](./visual-studio-code-extension.md#record-to-video-analyzer-video)
[cvr-with-grpcExtension](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/cvr-with-grpcExtension/topology.json) | Perform CVR. A subset of the video frames is sent to an external AI inference engine using the sharedMemory mode for data transfer via the gRPC extension. The results are then published to the IoT Edge Hub. | | [Record using gRPC Extension](./visual-studio-code-extension.md#record-using-grpc-extension)
[cvr-with-httpExtension](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/cvr-with-httpExtension/topology.json) | Perform CVR. A subset of the video frames is sent to an external AI inference engine via the HTTP extension. The results are then published to the IoT Edge Hub. | | [Record using HTTP Extension](./visual-studio-code-extension.md#record-using-http-extension)
[cvr-with-httpExtension-and-objectTracking](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/cvr-with-httpExtension-and-objectTracking/topology.json) | Perform CVR and track objects in a live feed. Inference metadata from an external AI inference engine is published to the IoT Edge Hub, and can be played back with the video. | [Record and stream inference metadata with video](edge/record-stream-inference-data-with-video.md) 
[cvr-with-motion](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/cvr-with-motion/topology.json) | Perform CVR. When motion is detected from a live video feed, relevant inferencing events are published to the IoT Edge Hub. | | [Record on motion detection](./visual-studio-code-extension.md#record-on-motion-detection)
[audio-video](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/audio-video/topology.json) | Perform CVR and record audio using the outputSelectors property. | | [Record audio with video](./visual-studio-code-extension.md#record-audio-with-video)

### Event-based video recording

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[evr-grpcExtension-video-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-grpcExtension-video-sink/topology.json)  | When an event of interest is detected by the external AI inference engine via the gRPC extension, those events are published to the IoT Edge Hub. The events are used to trigger the signal gate processor node that results in the appending of new clips to the Azure Video Analyzer video, corresponding to when the event of interest was detected. | [Develop and deploy gRPC inference server](edge/develop-deploy-grpc-inference-srv.md) | [Record using gRPC Extension](./visual-studio-code-extension.md#record-using-grpc-extension-1)
[evr-httpExtension-video-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-httpExtension-video-sink/topology.json) | When an event of interest is detected by the external AI inference engine via the HTTP extension, those events are published to the IoT Edge Hub. The events are used to trigger the signal gate processor node that results in the appending of new clips to the Azure Video Analyzer video, corresponding to when the event of interest was detected. | | [Record using HTTP Extension](./visual-studio-code-extension.md#record-using-http-extension-1)
[evr-hubMessage-video-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-hubMessage-video-sink/topology.json) | Use an object detection AI model to look for objects in the video, and record video clips only when a certain type of object is detected. The trigger to generate these clips is based on the AI inference events published onto the IoT Hub. | [Event-based video recording and playback](edge/record-event-based-live-video.md)| [Record to Video Analyzer video based on inference events](./visual-studio-code-extension.md#record-to-video-analyzer-video-based-on-inference-events)
[evr-hubMessage-file-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-hubMessage-file-sink/topology.json) | Record video clips to the local file system of the edge device whenever an external sensor sends a message to the pipeline topology. For example, the sensor can be a door sensor. | | [Record to local files based on inference events](./visual-studio-code-extension.md#record-to-local-files-based-on-inference-events)
[evr-motion-video-sink-file-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-motion-video-sink-file-sink/topology.json) | Perform event-based recording of video clips to the cloud and to the edge. When motion is detected from a live video feed, events are sent to a signal gate processor node that opens, allowing video to pass through to a file sink node and a video sink node. As a result, new files are created on the local file system of the Edge device, and new video clips are appended to your Video Analyzer video. The recordings contain the frames where motion was detected. | | [Record motion events to Video Analyzer video and local files](./visual-studio-code-extension.md#record-motion-events-to-video-analyzer-video-and-local-files) 
[evr-motion-video-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-motion-video-sink/topology.json) | When motion is detected, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger the signal gate processor node that will send frames to the video sink node when motion is detected. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. | [Detect motion, record video to Video Analyzer](edge/detect-motion-record-video-clips-cloud.md) | [Record motion events to Video Analyzer video](./visual-studio-code-extension.md#record-motion-events-to-video-analyzer-video)
[evr-motion-file-sink](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/evr-motion-file-sink/topology.json) | When motion is detected from a live video feed, events are sent to a signal gate processor node that opens, sending frames to a file sink node. As a result, new files are created on the local file system of the edge device, containing the frames where motion was detected. | [Detect motion and record video on edge devices](edge/detect-motion-record-video-edge-devices.md) | [Record motion events to local files](./visual-studio-code-extension.md#record-motion-events-to-local-files) 

### Motion detection

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[motion-detection](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/motion-detection/topology.json) | Detect motion in a live video feed. When motion is detected, those events are published to the IoT Hub. | [Get started with Azure Video Analyzer](edge/get-started-detect-motion-emit-events.md), [Get started with Video Analyzer in the portal](edge/get-started-detect-motion-emit-events-portal.md), [Detect motion and emit events](edge/detect-motion-emit-events-quickstart.md) | [Publish motion events to IoT Hub](./visual-studio-code-extension.md#publish-motion-events-to-iot-hub)
[motion-with-grpcExtension](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/motion-with-grpcExtension/topology.json) | Perform event-based recording in the presence of motion. When motion is detected from a live video feed, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger a signal gate processor node that will send frames to a video sink node only when motion is present. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. Additionally, run video analytics only when motion is detected. Upon detecting motion, a subset of the video frames is sent to an external AI inference engine via the gRPC extension. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - gRPC](edge/analyze-live-video-use-your-model-grpc.md) | [Analyzer motion events using gRPC Extension](./visual-studio-code-extension.md#analyze-motion-events-using-grpc-extension)
[motion-with-httpExtension](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/motion-with-httpExtension/topology.json) | Perform event-based recording in the presence of motion. When motion is detected in a live video feed, those events are published to the IoT Edge Hub. In addition, the motion events are used to trigger a signal gate processor node that will send frames to a video sink node only when motion is present. As a result, new video clips are appended to the Azure Video Analyzer video, corresponding to when motion was detected. Additionally, run video analytics only when motion is detected. Upon detecting motion, a subset of the video frames is sent to an external AI inference engine via the HTTP extension. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - HTTP](edge/analyze-live-video-use-your-model-http.md#generate-and-deploy-the-iot-edge-deployment-manifest) | [Analyze motion events using HTTP Extension](./visual-studio-code-extension.md#analyze-motion-events-using-http-extension)

### Extensions

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[grpcExtensionOpenVINO](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/grpcExtensionOpenVINO/topology.json) | Run video analytics on a live video feed. The gRPC extension allows you to create images at video frame rate from the camera that are converted to images, and sent to the [OpenVINO™ DL Streamer - Edge AI Extension module](https://aka.ms/ava-intel-ovms) provided by Intel. The results are then published to the IoT Edge Hub. | [Analyze live video with Intel OpenVINO™ DL Streamer – Edge AI Extension](edge/use-intel-grpc-video-analytics-serving-tutorial.md)
[httpExtension](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/httpExtension/topology.json) | Run video analytics on a live video feed. A subset of the video frames from the camera are converted to images, and sent to an external AI inference engine. The results are then published to the IoT Edge Hub. | [Analyze live video with your own model - HTTP](edge/analyze-live-video-use-your-model-http.md), [Analyze live video with Azure Video Analyzer on IoT Edge and Azure Custom Vision](edge/analyze-live-video-custom-vision.md) | [Analyze video using HTTP Extension](./visual-studio-code-extension.md#analyze-video-using-http-extension)
[httpExtensionOpenVINO](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/httpExtensionOpenVINO/topology.json) | Run video analytics on a live video feed. A subset of the video frames from the camera are converted to images, and sent to the [OpenVINO™ Model Server – AI Extension module](https://aka.ms/ava-intel-ovms) provided by Intel. The results are then published to the IoT Edge Hub. | [Analyze live video using OpenVINO™ Model Server – AI Extension from Intel](./edge/use-intel-openvino-tutorial.md) | [Analyze video with Intel OpenVINO Model Server](./visual-studio-code-extension.md#analyze-video-with-intel-openvino-model-server)

### Computer vision

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[spatial-analysis/person-count-operation-topology](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/spatial-analysis/person-count-operation-topology.json) | Live video is sent to an external [spatialAnalysis](../../cognitive-services/computer-vision/spatial-analysis-operations.md) module that counts people in a designated zone. When the criteria defined by the AI operation is met, events are sent to a signal gate processor that opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | [Person count operation with Computer Vision for Spatial Analysis](./visual-studio-code-extension.md#person-count-operation-with-computer-vision-for-spatial-analysis)
[spatial-analysis/person-line-crossing-operation-topology](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/spatial-analysis/person-line-crossing-operation-topology.json) | Live video is sent to an external [spatialAnalysis](../../cognitive-services/computer-vision/spatial-analysis-operations.md) module that tracks when a person crosses a designated line. When the criteria defined by the AI operation is met, events are sent to a signal gate processor that opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | [Person crossing line operation with Computer Vision for Spatial Analysis](./visual-studio-code-extension.md#person-crossing-line-operation-with-computer-vision-for-spatial-analysis)
[spatial-analysis/person-zone-crossing-operation-topology](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/spatial-analysis/person-zone-crossing-operation-topology.json) | Live video is sent to an external [spatialAnalysis](../../cognitive-services/computer-vision/spatial-analysis-operations.md) module that emits an event when a person enters or exists a zone. When the criteria defined by the AI operation is met, events are sent to a signal gate processor that opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | [Live Video with Computer Vision for Spatial Analysis](./edge/computer-vision-for-spatial-analysis.md) | [Person crossing zone operation with Computer Vision for Spatial Analysis](./visual-studio-code-extension.md#person-crossing-zone-operation-with-computer-vision-for-spatial-analysis)
[spatial-analysis/person-distance-operation-topology](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/spatial-analysis/person-distance-operation-topology.json) | Live video is sent to an external [spatialAnalysis](../../cognitive-services/computer-vision/spatial-analysis-operations.md) module that tracks when people violate a distance rule. When the criteria defined by the AI operation is met, events are sent to a signal gate processor that opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | [Person distance operation with Computer Vision for Spatial Analysis](./visual-studio-code-extension.md#person-distance-operation-with-computer-vision-for-spatial-analysis)
[spatial-analysis/custom-operation-topology](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/spatial-analysis/custom-operation-topology.json) | Live video is sent to an external [spatialAnalysis](../../cognitive-services/computer-vision/spatial-analysis-operations.md) module that carries out a supported AI operation. When the criteria defined by the AI operation is met, events are sent to a signal gate processor that opens, sending the frames to a video sink node. As a result, a new clip is appended to the Azure Video Analyzer video resource. | | [Custom operation with Computer Vision for Spatial Analysis](./visual-studio-code-extension.md#custom-operation-with-computer-vision-for-spatial-analysis)

### AI composition

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[ai-composition](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/ai-composition/topology.json) | Run 2 AI inferencing models of your choice. In this example, classified video frames are sent from an AI inference engine using the [Tiny YOLOv3 model](https://github.com/Azure/video-analyzer/tree/main/edge-modules/extensions/yolo/tinyyolov3/grpc-cpu) to another engine using the [YOLOv3 model](https://github.com/Azure/video-analyzer/tree/main/edge-modules/extensions/yolo/yolov3/grpc-cpu). Having such a topology enables you to trigger a heavy AI module, only when a light AI module indicates a need to do so. | [Analyze live video streams with multiple AI models using AI composition](edge/analyze-ai-composition.md) | [Record to the Video Analyzer service using multiple AI models](./visual-studio-code-extension.md#record-to-the-video-analyzer-service-using-multiple-ai-models)

### Miscellaneous

Name | Description | Samples | VSCode Name
:----- | :----  | :---- | :---
[object-tracking](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/object-tracking/topology.json) | Track objects in a live video feed. The object tracker comes in handy when you need to detect objects in every frame, but the edge device does not have the necessary compute power to be able to apply the vision model on every frame. | [Track objects in a live video](edge/track-objects-live-video.md) | [Record video based on the object tracking AI model](./visual-studio-code-extension.md#record-video-based-on-the-object-tracking-ai-model) 
[line-crossing](https://github.com/Azure/video-analyzer/blob/main/pipelines/live/topologies/line-crossing/topology.json) | Use a computer vision model to detect objects in a subset of frames when they cross a virtual line in a live video feed. The object tracker node is used to track those objects in the frames and pass them through a line-crossing node. The line-crossing node comes in handy when you want to detect objects that cross the imaginary line and emit events. | [Detect when objects cross a virtual line in a live video](edge/use-line-crossing.md) | [Record video based on the line crossing AI model](./visual-studio-code-extension.md#record-video-based-on-the-line-crossing-ai-model)

## Next steps

[Understand Video Analyzer pipelines](pipeline.md).