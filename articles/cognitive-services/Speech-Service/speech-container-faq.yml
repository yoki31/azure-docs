### YamlMime:FAQ
metadata:
  title: Speech service containers frequently asked questions (FAQ)
  titleSuffix: Azure Cognitive Services
  description: Install and run containers for speech-to-text and text-to-speech.
  services: cognitive-services
  author: eric-urban
  manager: nitinme
  ms.service: cognitive-services
  ms.subservice: speech-service
  ms.topic: conceptual
  ms.date: 01/24/2022
  ms.author: eur
  ms.custom: devx-track-csharp
    
title: Speech service containers frequently asked questions (FAQ)
summary: When using the Speech service with containers, you can refer to these frequently asked questions before contacting support. This article captures questions of varying degree, from general to technical. 


sections:
  - name: General questions
    questions:
      - question: |
          How do Speech containers work and how do I set them up?
        answer: |
          For an overview, see [Install and run Speech containers](speech-container-howto.md). When setting up the production cluster, there are several things to consider. First, setting up single language, multiple containers, on the same machine, should not be a large issue. If you are experiencing problems, it may be a hardware-related issue - so we would first look at resource, that is; CPU and memory specifications.
          
          Consider for a moment, the `ja-JP` container and latest model. The acoustic model is the most demanding piece CPU-wise, while the language model demands the most memory. When we benchmarked the use, it takes about 0.6 CPU cores to process a single speech-to-text request when audio is flowing in at real-time, for example, from a microphone. If you are feeding audio faster than real-time, for example, from a file, that usage can double (1.2x cores). Meanwhile, the memory in this context is operating memory for decoding speech. It does *not* take into account the actual full size of the language model, which will reside in file cache. It's an additional 2 GB for `ja-JP`; for `en-US`, it may be more (6-7 GB).
          
          If you have a machine where memory is scarce, and you are trying to deploy multiple languages on it, it is possible that file cache is full, and the OS is forced to page models in and out. For a running transcription, that could be disastrous, and may lead to slowdowns and other performance implications.
          
          Furthermore, we pre-package executables for machines with the [advanced vector extension (AVX2)](speech-container-howto.md#advanced-vector-extension-support) instruction set. A machine with the AVX512 instruction set will require code generation for that target, and starting 10 containers for 10 languages may temporarily exhaust CPU. A message like this one will appear in the docker logs:
          
          ```console
          2020-01-16 16:46:54.981118943 
          [W:onnxruntime:Default, tvm_utils.cc:276 LoadTVMPackedFuncFromCache]
          Cannot find Scan4_llvm__mcpu_skylake_avx512 in cache, using JIT...
          ```
          
          You can set the number of decoders you want inside a *single* container using `DECODER MAX_COUNT` variable. Start with your CPU and memory SKU and then refer to the recommended host machine resource specifications.  
          
      - question: |
          Could you help with capacity planning and cost estimation of on-prem Speech-to-text containers?
        answer: |
          For container capacity in batch processing mode, each decoder could handle 2-3x in real-time, with two CPU cores, for a single recognition. We do not recommend keeping more than two concurrent recognitions per container instance, but recommend running more instances of containers for reliability and availability reasons, behind a load balancer.
          
          Though we could have each container instance running with more decoders. For example, we may be able to set up seven decoders per container instance on an eight-core machine at more than 2x each, yielding 15x throughput. There is a param `DECODER_MAX_COUNT` to be aware of. For the extreme case, reliability and latency issues arise, with throughput increased significantly. For a microphone, it will be at 1x real-time. The overall usage should be at about one core for a single recognition.
          
          For scenario of processing 1-K hours per day in batch processing mode, in an extreme case, 3 VMs could handle it within 24 hours but not guaranteed. To handle spike days, failover, update, and to provide minimum backup/BCP, we recommend 4-5 machines instead of 3 per cluster, and with 2+ clusters.
          
          For hardware, we use standard Azure VM `DS13_v2` as a reference (each core must be 2.6 GHz or better, with AVX2 instruction set enabled).
          
          | Instance  | vCPU(s) | RAM    | Temp storage | Pay-as-you-go with AHB | 1-year reserve with AHB (% Savings) | 3-year reserved with AHB (% Savings) |
          |-----------|---------|--------|--------------|------------------------|-------------------------------------|--------------------------------------|
          | `DS13 v2` | 8       | 56 GiB | 112 GiB      | $0.598/hour            | $0.3528/hour (~41%)                 | $0.2333/hour (~61%)                  |
          
          Based on the design reference (two clusters of 5 VMs to handle 1 K hours/day audio batch processing), one-year hardware cost will be:
          
          > 2 (clusters) * 5 (VMs per cluster) * $0.3528/hour * 365 (days) * 24 (hours) = $31 K / year
          
          When mapping to physical machine, a general estimation is 1 vCPU = 1 Physical CPU Core. In reality, 1vCPU is more powerful than a single core.
          
          For on-prem, all of these additional factors come into play:
          
          - On what type the physical CPU is and how many cores on it
          - How many CPUs running together on the same box/machine
          - How VMs are set up
          - How hyper-threading / multi-threading is used
          - How memory is shared
          - The OS, etc.
          
          Normally it is not as well tuned as Azure the environment. Considering other overhead, one estimation is 10 physical CPU cores = 8 Azure vCPU. Though popular CPUs only have eight cores. With on-prem deployment, the cost will be higher than using Azure VMs. Also, consider the depreciation rate.
          
          Service cost is the same as the online service: $1/hour for speech-to-text. The Speech service cost is:
          
          > $1 * 1000 * 365 = $365 K
          
          Maintenance cost paid to Microsoft depends on the service level and content of the service. People cost is not included. Other infrastructure costs such as storage, networks, and load balancers are not included.

          
      - question: |
          Can I use a custom acoustic model and language model with Speech container?
        answer: |
          One model ID can be used, either a custom language model or custom acoustic model. Passing acoustic and language models concurrently is not supported.
          
      - question: |
          Could you explain these errors from the custom speech-to-text container?
        answer: |
          **Error 1:**
          
          ```cmd
          Failed to fetch manifest: Status: 400 Bad Request Body:
          {
              "code": "InvalidModel",
              "message": "The specified model is not supported for endpoint manifests."
          }
          ```
          
          If you're training with the latest custom model, we currently don't support that. If you train with an older version, it should be possible to use. We are still working on supporting the latest versions.
          
          Essentially, the custom containers do not support Halide or ONNX-based acoustic models (which is the default in the custom training portal). This is due to custom models not being encrypted and we don't want to expose ONNX models, however; language models are fine. The model size may be larger by 100 MB.
          
          **Error 2:**
          
          ```cmd
          HTTPAPI result code = HTTPAPI_OK.
          HTTP status code = 400.
          Reason:  Synthesis failed.
          StatusCode: InvalidArgument,
          Details: Voice does not match.
          ```
          
          You need to provide the correct voice name in the request, which is case-sensitive. Refer to the full service name mapping.
          
          **Error 3:**
          
          ```json
          {
              "code": "InvalidProductId",
              "message": "The subscription SKU \"CognitiveServices.S0\" is not supported in this service instance."
          }
          ```
          
          You reed to create a Speech resource, not a Cognitive Services resource.
          
          
      - question: |
          What API protocols are supported, REST or WS?
        answer: |
          For speech-to-text and custom speech-to-text containers, we currently only support the websocket based protocol. The SDK only supports calling in WS but not REST. Always refer to the official documentation, see [query prediction endpoints](speech-container-howto.md#query-the-containers-prediction-endpoint).
          
      - question: |
          Why am I getting errors when attempting to call LUIS prediction endpoints?
        answer: |
          I am using the LUIS container in an IoT Edge deployment and am attempting to call the LUIS prediction endpoint from another container. The LUIS container is listening on port 5001, and the URL I'm using is this:
          
          ```csharp
          var luisEndpoint =
              $"ws://192.168.1.91:5001/luis/prediction/v3.0/apps/{luisAppId}/slots/production/predict";
          var config = SpeechConfig.FromEndpoint(new Uri(luisEndpoint));
          ```
          
          The error I'm getting is:
          
          ```cmd
          WebSocket Upgrade failed with HTTP status code: 404 SessionId: 3cfe2509ef4e49919e594abf639ccfeb
          ```
          
          I see the request in the LUIS container logs and the message says:
          
          ```cmd
          The request path /luis//predict" does not match a supported file type.
          ```        
          
          The Speech SDK should not be used for a LUIS container. For using the LUIS container, the LUIS SDK or LUIS REST API should be used. Speech SDK should be used for a speech container.
          
          A cloud is different than a container. A cloud can be composed of multiple aggregated containers. In this context there are two separate LUIS and Speech containers deployed in the cloud. The Speech container only does speech. The LUIS container only does LUIS. Performance would be slow for a remote client to go to the cloud, do speech, come back, then go to the cloud again and do LUIS. So we provide a feature that allows the client to go to Speech, stay in the cloud, go to LUIS then come back to the client. Thus even in this scenario the Speech SDK goes to Speech cloud container with audio, and then Speech cloud container talks to LUIS cloud container with text. The LUIS container has no concept of accepting audio. Since LUIS is a text-based service, a LUIS container doesn't accept streaming audio. With on-prem, we have no certainty our customer has deployed both containers, we don't presume to orchestrate between containers in our customers' premises, and if both containers are deployed on-prem, given they are more local to the client, it is not a burden to go the SR first, back to client, and have the customer then take that text and go to LUIS.         
          
      - question: |
          How can we benchmark a rough measure of transactions/second/core?
        answer: |
          Here are some of the rough numbers to expect from the model prior to general availability (GA):
          
          - For files, the throttling will be in the Speech SDK, at 2x. First five seconds of audio are not throttled. Decoder is capable of doing about 3x real-time. For this, the overall CPU usage will be close to two cores for a single recognition.
          - For mic, it will be at 1x real-time. The overall usage should be at about one core for a single recognition.
          
          This can all be verified from the docker logs. We actually dump the line with session and phrase/utterance statistics, and that includes the RTF numbers.
          
      - question: |
          How do I make multiple containers run on the same host?
        answer: |
          By setting each container to listen on a different port using the -p argument. Use `-p <outside_unique_port>:5000`. For example: `-p 5001:5000`.
          
  - name: Technical questions
    questions:
      - question: |
          How can I get non-batch APIs to handle audio <15 seconds long?
        answer: |
          The `RecognizeOnce()` SDK operation in interactive mode processes up to 15 seconds of audio where utterances are expected to be short. You use `StartContinuousRecognition()` for dictation or conversation longer than 15 seconds.
          
          
      - question: |
          What are the recommended resources, CPU and RAM; for 50 concurrent requests?
        answer: |
          How many concurrent requests will a 4-core, 4-GB RAM handle? If we have to serve for example, 50 concurrent requests, how many Core and RAM is recommended?
          
          At real-time, 8 with our latest `en-US`, so we recommend using more docker containers beyond six concurrent requests. Beyond 16 cores it becomes non-uniform memory access (NUMA) node sensitive. The following table describes the minimum and recommended allocation of resources for each Speech container.

          # [Speech-to-text](#tab/stt)
         
          | Container      | Minimum             | Recommended         |
          |----------------|---------------------|---------------------|
          | Speech-to-text | 2-core, 2-GB memory | 4-core, 4-GB memory |
         
          # [Custom Speech-to-text](#tab/cstt)
         
          | Container             | Minimum             | Recommended         |
          |-----------------------|---------------------|---------------------|
          | Custom Speech-to-text | 2-core, 2-GB memory | 4-core, 4-GB memory |
         
          # [Text-to-speech](#tab/tts)
         
          | Container      | Minimum             | Recommended         |
          |----------------|---------------------|---------------------|
          | Text-to-speech | 1-core, 2-GB memory | 2-core, 3-GB memory |
         
          # [Custom Text-to-speech](#tab/ctts)
         
          | Container             | Minimum             | Recommended         |
          |-----------------------|---------------------|---------------------|
          | Custom Text-to-speech | 1-core, 2-GB memory | 2-core, 3-GB memory |
         
          ***

          - Each core must be at least 2.6 GHz or faster.
          - For files, the throttling will be in the Speech SDK, at 2x. The first 5 seconds of audio are not throttled.
          - The decoder is capable of doing about 2-3x real-time. For this, the overall CPU usage will be close to two cores for a single recognition. That's why we do not recommend keeping more than two active connections, per container instance. The extreme side would be to put about 10 decoders at 2x real-time in an eight-core machine like `DS13_V2`. For the container version 1.3 and later, there's a param you could try setting `DECODER_MAX_COUNT=20`.
          - For microphone, it will be at 1x real-time. The overall usage should be at about one core for a single recognition.
         
          Consider the total number of hours of audio you have. If the number is large, to improve reliability and availability, we suggest running more instances of containers, either on a single box or on multiple boxes, behind a load balancer. Orchestration could be done using Kubernetes (K8S) and Helm, or with Docker compose.
         
          As an example, to handle 1000 hours/24 hours, we have tried setting up 3-4 VMs, with 10 instances/decoders per VM.

      - question: |
          Does the Speech container support punctuation?
        answer: |
          We have capitalization (ITN) available in the on-prem container. Punctuation is language-dependent, and not supported for some languages, including Chinese and Japanese.
          
          We *do* have implicit and basic punctuation support for the existing containers, but it is `off` by default. What that means is that you can get the `.` character in your example, but not the `。` character. To enable this implicit logic, here's an example of how to do so in Python using our Speech SDK. It would be similar in other languages:
          
          ```python
          speech_config.set_service_property(
              name='punctuation',
              value='implicit',
              channel=speechsdk.ServicePropertyChannel.UriQueryParameter
          )
          ```
          
      - question: |
          Why am I getting 404 errors when attempting to POST data to speech-to-text container?
        answer: |        
          Speech-to-text containers do not support REST API. The Speech SDK uses WebSockets. Always refer to the official documentation, see [query prediction endpoints](speech-container-howto.md#query-the-containers-prediction-endpoint).
          
          
      - question:  Why is the container running as a non-root user? What issues might occur because of this?
        answer: |
          Note that the default user inside the container is a non-root user. This provides protection against processes escaping the container and obtaining escalated permissions on the host node. By default, some platforms like the OpenShift Container Platform already do this by running containers using an arbitrarily assigned user ID. For these platforms, the non-root user will need to have permissions to write to any externally mapped volume that requires writes. For example a logging folder, or a custom model download folder.
      - question: |
          When using the speech-to-text service, why am I getting this error?
        answer: |
          ```cmd
          Error in STT call for file 9136835610040002161_413008000252496:
          {
              "reason": "ResultReason.Canceled",
              "error_details": "Due to service inactivity the client buffer size exceeded. Resetting the buffer. SessionId: xxxxx..."
          }
          ```
          
          Cancellation can occur when the audio feed exceeds the Speech container buffer size. You need to control the concurrency and the Real-Time Factor (RTF) at which you send the audio. The RTF can fluctuate depending on concurrent requests, CPU, and memory allocated.

          
additionalContent: |

  ## Next steps
  
  > [!div class="nextstepaction"]
  > [Cognitive Services containers](speech-container-howto.md)
